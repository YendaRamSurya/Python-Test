{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistics for Quantitative Trading\n",
    "<div class=\"alert alert-info\"><strong>Part II : Time Series Modeling with Python</strong></div>\n",
    "\n",
    "## Draft Version (Final version will be uploaded in a couple of days)\n",
    "### *Everything until the topic Testing for stationarity is complete. The remaining parts are incomplete or in draft form.*\n",
    "#### Notebook Created on: 6 July 2020\n",
    "##### Last Update: 9 Jul 2020\n",
    "##### Version 1.0\n",
    "##### Author: Vivek Krishnamoorthy\n",
    "\n",
    "## **Agenda for today**\n",
    "- Anatomy of a time series process\n",
    "- Modeling time series using decomposition\n",
    "    - Method I: `statsmodels` library\n",
    "    - Method II: `fbprophet` library\n",
    "- Testing for stationarity\n",
    "- Modeling time series using\n",
    "    - Method III: Exponential smoothing\n",
    "    - Method IV: ARIMA \n",
    "- Analyzing stock return volatility with ARCH/GARCH models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'convert'></a>\n",
    "\n",
    "### Anatomy of a time series process\n",
    "\n",
    "It is helpful to consider a time series process as a combination of *systematic* and *unsystematic* components.\n",
    "\n",
    "- **Systematic**: These are recurring in nature and so can be described and modeled.\n",
    "- **Non-systematic**: These are random in nature and so cannot be directly modeled.\n",
    "\n",
    "The systematic components can be further split into *level*, *trend*, and *seasonality* whereas the non-systematic component is referred to as *noise*.\n",
    "\n",
    "- **Level**: The average value of the process.\n",
    "- **Trend**: The direction and rate of change of the process. The slope is a good proxy for it.\n",
    "- **Seasonality**: Deviations in the process caused by recurring short-term cycles.\n",
    "- **Noise**: The random variation observed in the process.\n",
    "\n",
    "Another useful abstraction while analyzing time series processes is to see them as either an *additive* or a *multiplicative* blend of the four constituent parts mentioned.\n",
    "\n",
    "**Additive model**: The process $X(t)$ has the form\n",
    "$$X(t) = Level + Trend + Seasonality + Noise$$\n",
    "\n",
    "We use an additive model when the underlying process under examination has the following characteristics.\n",
    "- The process changes remain constant over time (i.e. they are linear). So the trend line would be straight.\n",
    "- It shows linear seasonality. That is to say the frequency and amplitude (i.e. the width and the height) of the cycles remain constant over time.\n",
    "\n",
    "\n",
    "\n",
    "**Multiplicative model**: The process $X(t)$ has the form\n",
    "$$X(t) = Level \\times Trend \\times Seasonality \\times Noise$$\n",
    "\n",
    "We use a multiplicative model when the underlying process under examination has the following characteristics.\n",
    "- The process changes vary over time (i.e. they are non-linear in nature).\n",
    "- An exponential or quadratic or higher order polynomial process is multiplicative. So the trend-line would be curved, not straight. \n",
    "- It shows non-linear seasonality. That is to say the frequency and amplitude (i.e. the width and the height) of the cycles vary over time.\n",
    "\n",
    "In the (harsh) real world, we often encounter non-linear or even mixed processes and therefore have to work with the multiplicative model as our prototype. But we prefer to work with linear processes as they are easier to use. So we transform the original process into a linear one. A commonly used trick is applying a log transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling a time series using decomposition\n",
    "\n",
    "There are several available libraries (ex. [`statsmodels`](https://www.statsmodels.org/stable/tsa.html), [`fbprophet`](https://facebook.github.io/prophet/docs/quick_start.html), [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), [`PyFlux`](https://pyflux.readthedocs.io/en/latest/), [`fecon236`](https://github.com/MathSci/fecon236), [`PM-Prophet`](https://github.com/luke14free/pm-prophet) at the time of this writing) in Python to develop sophisticated time series models for forecasting. \n",
    "\n",
    "We work with `statsmodels` and `fbprophet` here. Both offer convenient routines to automatically decompose a time series into their fundamental components.\n",
    "\n",
    "We will work with the daily crude oil (from 2003 to 2020) and soy bean (from 2000 to 2020) price series.\n",
    "\n",
    "We first try the `seasonal_decompose` method from the `statsmodels.tsa` sub-library and then experiment with the `fbprophet` library.\n",
    "\n",
    "#### Method I : Seasonal decomposition using the `seasonal_decompose` routine in `statsmodels` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-23T17:16:59.406816Z",
     "iopub.status.busy": "2020-07-23T17:16:59.406816Z",
     "iopub.status.idle": "2020-07-23T17:16:59.941092Z",
     "shell.execute_reply": "2020-07-23T17:16:59.940086Z",
     "shell.execute_reply.started": "2020-07-23T17:16:59.406816Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import quandl\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-23T17:20:54.671090Z",
     "iopub.status.busy": "2020-07-23T17:20:54.670090Z",
     "iopub.status.idle": "2020-07-23T17:20:54.673232Z",
     "shell.execute_reply": "2020-07-23T17:20:54.673090Z",
     "shell.execute_reply.started": "2020-07-23T17:20:54.671090Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open(\"quandl_apikey.txt\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-23T17:18:33.042361Z",
     "iopub.status.busy": "2020-07-23T17:18:33.042361Z",
     "iopub.status.idle": "2020-07-23T17:18:33.068888Z",
     "shell.execute_reply": "2020-07-23T17:18:33.067896Z",
     "shell.execute_reply.started": "2020-07-23T17:18:33.042361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quandl_apikey file not found\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c955e219a5d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m## Garbled characters appear at the start of my API key, hence the below step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mquandl_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquandl_key\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mquandl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mApiConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquandl_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "################### TO STUDENTS: PLEASE IGNORE THIS CELL ####################\n",
    "#############################################################################\n",
    "\n",
    "## I do the below procedure so as to not show my API key.\n",
    "## You can choose to ignore it.\n",
    "\n",
    "filename = \"quandl_apikey.txt\"\n",
    "\n",
    "def get_file_contents(filename):\n",
    "    \"\"\" If provided a filename,\n",
    "        this function returns the contents of that file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            # We create a file containing a single line,\n",
    "            # with our Quandl API key\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{filename} file not found\")\n",
    "\n",
    "quandl_key = get_file_contents(filename)\n",
    "\n",
    "## Garbled characters appear at the start of my API key, hence the below step\n",
    "quandl_key = quandl_key[3:]\n",
    "\n",
    "quandl.ApiConfig.api_key = quandl_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################### TO STUDENTS: PLEASE RUN THIS CELL #######################\n",
    "#############################################################################\n",
    "\n",
    "## If you have installed quandl and created a free account there, you would have an API key.\n",
    "## Please copy-paste below as shown and replace YOURAPIKEY with your key from quandl.\n",
    "## Then uncomment the below line and run the cell.\n",
    "# quandl.ApiConfig.api_key = \"YOURAPIKEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(\"crude_oil_prices.csv\")\n",
    "# df4 = pd.read_csv(\"crude_oil_prices.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start4 = '2003-01-01'\n",
    "end4 = '2020-07-05'\n",
    "ticker4 = \"OPEC/ORB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ***********************************************************\n",
    "## ***** EXPERIMENTAL : IGNORE THIS CELL *********************\n",
    "## Trials with other commodity data\n",
    "## ***********************************************************\n",
    "\n",
    "# \"OPEC/ORB\" this is crude oil prices per barrel\n",
    "# WGC/GOLD_DAILY_INR for daily gold prices in India available until 10 March 2020\n",
    "# TFGRAIN/SOYBEANS for daily soy bean prices per bushel\n",
    "# WORLDAL/PALPROD primary aluminium production across continents. not tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = quandl.get(dataset=ticker4, start_date=start4, end_date=end4)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker4} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df4.head(10))\n",
    "print(df4.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.rename(columns={'Value': 'price'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.plot(figsize=(12, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- There are upward and downward trends in the prices. Looks linear. Needs further probing.\n",
    "- There seems to be seasonality and we can investigate further by looking at some moving averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 21 # for 1 monthly moving average calculations\n",
    "window_length2 = 252 # for annual moving average calculations\n",
    "\n",
    "## Calculating 21-day rolling mean and volatility\n",
    "\n",
    "df4['rolling_21d_mean'] = df4['price'].rolling(window=window_length).mean()\n",
    "df4['rolling_21d_vol'] = df4['price'].rolling(window=window_length).std()\n",
    "\n",
    "\n",
    "## Calculating 252-day rolling mean and volatility\n",
    "\n",
    "df4['rolling_12m_mean'] = df4['price'].rolling(window=window_length2).mean()\n",
    "df4['rolling_12m_vol'] = df4['price'].rolling(window=window_length2).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.25)\n",
    "df4.plot(figsize=(12, 9))\n",
    "\n",
    "plt.title(\"OPEC benchmark crude oil prices over time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price per barrel (in US$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The yearly moving average of the prices show a linear trend (which changes roughly every couple of years).\n",
    "- The monthly moving price average shows seasonality.\n",
    "- The rolling volatility is time-varying in both (monthly and annual) cases.\n",
    "- Let's try using the **multiplicative** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method I: Using statsmodels\n",
    "\n",
    "decompose_series = seasonal_decompose(df4['price'], period=252, model=\"multiplicative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, figsize=(15, 10))\n",
    "\n",
    "decompose_series.observed.plot(ax=ax[0])\n",
    "ax[0].set_title(\"Time series of crude oil prices\", fontsize=16)\n",
    "ax[0].set(xlabel=\"\", ylabel=\"Oil price (in US$/barrel)\")\n",
    "\n",
    "decompose_series.trend.plot(ax=ax[1])\n",
    "ax[1].set(xlabel=\"\", ylabel=\"Trend\")\n",
    "\n",
    "decompose_series.seasonal.plot(ax=ax[2])\n",
    "ax[2].set(xlabel=\"\", ylabel=\"Seasonal\")\n",
    "\n",
    "decompose_series.resid.plot(ax=ax[3])\n",
    "ax[3].set(xlabel=\"Date\", ylabel=\"Residual\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The plot shows us the observed, trend, seasonal and residual time series. We can access each component by typing: `decompose_series.trend`, `decompose_series.seasonal`, and `decompose_series.residual` \n",
    "- The trend and seasonal plots that have been extracted from the original series look plausible.\n",
    "- The residual plot clearly has non-constant volatility. If the model was a suitable fit, then after taking out the trend and seasonality present in the price data, we would have residuals that do not have any discernable pattern. Not so here.\n",
    "- At this stage, we would evaluate alternatives to model the residuals. We could even consider exogenous variables like oil production, renewable energy investments, etc. which would influence oil prices (outside the scope of this session).\n",
    "- From the `statsmodels` documentation: *This is a naive decomposition. More sophisticated methods should be preferred.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method II : Seasonal decomposition using Facebook's Prophet library `fbprophet` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method II: Using fbprophet\n",
    "## Working with the soy bean price series\n",
    "\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start5 = '2000-12-01'\n",
    "end5 = '2020-02-29'\n",
    "ticker5 = \"TFGRAIN/SOYBEANS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = quandl.get(dataset=ticker5, start_date=start5, end_date=end5)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker5} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(\"soy_bean_prices.csv\")\n",
    "# df4 = pd.read_csv(\"soy_bean_prices.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5[['Cash Price', 'Fall Price', 'Basis', 'Fall Basis']].plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5[['Cash Price', 'Fall Price', 'Basis']].plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- We will only work with `Cash Price` for the forecasting. There are both increasing and decreasing trends in the cash prices.\n",
    "- Since soy bean grains are an agricultural commodity, we see seasonality through the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.drop(columns=['Basis', 'Fall Price', 'Fall Basis'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following the DataFrame manipulation procedure and further steps\n",
    "## as recommended in Facebook's Prophet documentation\n",
    "df5.reset_index(drop=False, inplace=True)\n",
    "df5.rename(columns={'Date': 'ds', 'Cash Price': 'y'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We split the data into train and test sets (this is the language of machine learning)\n",
    "## In the language of econometrics, we would call them in-sample and out-of-sample\n",
    "## Here, we choose the index for the training data set\n",
    "\n",
    "train_index = df5['ds'].apply(lambda x: x.year) < 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the training set\n",
    "\n",
    "df5_train = df5.loc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for NaNs\n",
    "\n",
    "df5_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_train.head(), df5_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the testing set\n",
    "\n",
    "df5_test = df5.loc[~train_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_test.head(), df5_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.shape, df5_train.shape, df5_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Instantiating the Prophet model\n",
    "## By default, it is 'additive'.\n",
    "## On your own time, you can try 'multiplicative' while reviewing the material\n",
    "model1 = Prophet(seasonality_mode='additive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Additional specification that our data has monthly seasonality\n",
    "## Other arguments have been set as per the documentation\n",
    "model1.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "\n",
    "## The below method is similar to the scikit-learn library's fit().\n",
    "## The model is fitted using the training data specified earlier.\n",
    "model1.fit(df5_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df5_future = model1.make_future_dataframe(periods=425)\n",
    "df5_pred = model1.predict(df5_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_future.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_future.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_future.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_pred.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the predicted values on testing and training data\n",
    "\n",
    "model1.plot(df5_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The model's predicted values (blue line) approximately follow the observed soy bean prices (black dots).\n",
    "- The light blue shadow is the confidence interval for the predicted values. Its width changes over time and quantifies our confidence in the point estimates.\n",
    "- Visually we see that whenever there's a sharp change in prices, the model fails to predict correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.plot_components(df5_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The overall trend shows an increase in cash prices over the entire time period.\n",
    "- In the weekly plot, we ignore the prices on weekends (no trading). The prices through the week are quite constant (i.e. no day-of-the-week effect).\n",
    "- In the yearly plot, there's a wide range of prices (of ~US\\$ 1 with a high in July and a low in October) observed across the year. We would expect to see seasonality in an agricultural commodity like soy beans (month-of-the=-year effect).\n",
    "- In the monthly plot, we see some variance but the scale is much smaller than in the previous plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## We create a merged DataFrame to examine the actuals v/s predicted values closely\n",
    "\n",
    "selected_columns = ['ds', 'yhat_lower', 'yhat_upper', 'yhat']\n",
    "df5_pred = df5_pred.loc[:, selected_columns].reset_index(drop=True)\n",
    "\n",
    "## Using left join, we only select rows that are part of our testing data set.\n",
    "## This would exclude the predictions on holidays and weekends.\n",
    "\n",
    "df5_test = df5_test.merge(df5_pred, on=['ds'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_test['ds'] = pd.to_datetime(df5_test['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_test.set_index('ds', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax = sns.lineplot(data=df5_test[['y', 'yhat_lower', \n",
    "                                 'yhat_upper', 'yhat']])\n",
    "ax.fill_between(df5_test.index, df5_test.yhat_lower, \n",
    "                df5_test.yhat_upper, alpha=0.3)\n",
    "ax.set(title='Soy bean price - actual vs. predicted', \n",
    "       xlabel='Date', ylabel='Price per bushel (US$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- The interval estimate of the soy bean price prediction appears to have been accurate for the whole period (except a month in May 2019).\n",
    "- The confidence interval in the last six months is wider than in the initial period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for stationarity\n",
    "\n",
    "There are three ways of checking for stationarity in a time series.\n",
    "1. Visual inspection\n",
    "2. Statistical tests\n",
    "3. ACF/PACF plots\n",
    "\n",
    "We prefer working with stationary time series processes because it makes modeling, analysis and forecasting more feasible.\n",
    "\n",
    "For this section, we work with daily gold prices in India. The prices shown are denominated in INR per ounce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start6 = '1995-01-01'\n",
    "end6 = '2020-02-29'\n",
    "ticker6 = \"WGC/GOLD_DAILY_INR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = quandl.get(dataset=ticker6, start_date=start6, end_date=end6)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker6} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(\"gold_prices_inr.csv\")\n",
    "# df4 = pd.read_csv(\"gold_prices_inr.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.rename(columns={\"Value\": \"price\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats as sms\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(y, wl1=21, wl2=252, lags=40, figsize=(15, 10)):\n",
    "    \"\"\" Checks the stationarity of a pandas Series,\n",
    "        using plots, correlograms and the ADF test\n",
    "    \"\"\"\n",
    "    ## Calculating rolling statistics\n",
    "    \n",
    "    rolling_wl1_mean = y.rolling(window=wl1).mean()\n",
    "    rolling_wl2_mean = y.rolling(window=wl2).mean()\n",
    "    rolling_wl1_vol = y.rolling(window=wl1).std()\n",
    "    rolling_wl2_vol = y.rolling(window=wl2).std()\n",
    "    \n",
    "    ## Plotting the price, rolling statistics and correlograms\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    sns.set(font_scale=1)\n",
    "    layout = (2, 2)\n",
    "    y_ax = plt.subplot2grid(layout, (0, 0))\n",
    "    vol_ax = plt.subplot2grid(layout, (0, 1))\n",
    "    acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "    pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "        \n",
    "    y.plot(ax=y_ax)\n",
    "    rolling_wl1_mean.plot(ax=y_ax)\n",
    "    rolling_wl2_mean.plot(ax=y_ax)\n",
    "    \n",
    "    rolling_wl1_vol.plot(ax=vol_ax)\n",
    "    rolling_wl2_vol.plot(ax=vol_ax)\n",
    "    y_ax.set_title('Rolling means over time')\n",
    "    y_ax.legend(['observed', f'{wl1}-day MA of observed', f'{wl2}-day MA of observed'], loc='best')\n",
    "    #y_ax.set_ylabel(\"Gold prices(in INR)/oz.\")\n",
    "    \n",
    "    vol_ax.set_title('Rolling volatility over time')\n",
    "    vol_ax.legend([f'{wl1}-day MA of volatility', f'{wl2}-day MA of volatility'], loc='best')\n",
    "    \n",
    "    sm.graphics.tsa.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.05)\n",
    "    sm.graphics.tsa.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.05)\n",
    "    \n",
    "    ## Running the Augmented Dickey-Fuller test\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('--------- The augmented Dickey-Fuller test results -----------')\n",
    "    print('--------------------------------------------------------------')\n",
    "    adftest = adfuller(y, autolag='AIC')\n",
    "    results = pd.Series(adftest[0:4], index=['Test Statistic','p-value','# of Lags','# of Observations'])\n",
    "    for key,value in adftest[4].items():\n",
    "        results[f'Critical Value ({key})'] = '{0:.3f}'.format(value)\n",
    "    print(results)\n",
    "    print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(df6['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- In the ADF test, if the test statistic is greater than the critical value, we conclude that the series is non-stationary. We can draw the same conclusion by examining the p-value. A p-value greater than our significance level (conventionally 5%) means we cannot reject our null hypothesis (The series is not stationary). \n",
    "- For the gold prices, we have a p-value of nearly 1 (and equivalently the test statistic is greater than the critical values at all 3 significance levels), so we conclude that the price series is not stationary.\n",
    "- The rolling means and volatility plots are time-varying. So we also conclude visually that gold prices in India are non-stationary.\n",
    "- From the ACF, there are significant autocorrelations above the 95% confidence interval at all lags. From the PACF, we have significance in autocorrelations at lags 1, 2, 3, 6, and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6['log_returns'] = np.log(df6['price'] / df6['price'].shift(1))\n",
    "df6.dropna(axis='rows', how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(df6['log_returns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- As per the ADF test results, the returns of gold are stationary since the p-value is almost 0 and the test statistic is less than all the critical values.\n",
    "- The returns and rolling means of the returns are all centred around 0. As the time scale increases, the means become more and more constant. At shorter time scales, the noise tends to obscure the signal.\n",
    "- The volatily is time-varying at both the faster and slower rolling levels.\n",
    "- There is a little spike in the ACF plot at lags 3, 11, and 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method III : Modeling time series using exponential smoothing\n",
    "\n",
    "This method works well with non-stationary data. It is similar to the exponential moving averages in that higher weights are assigned to more recent data. It is an alternative to the ARIMA class of methods (which we will explore later).\n",
    "\n",
    "In exponential smoothing, the forecasts are a weighted sum of past observations wherein the weights decrease exponentially as we move further into the past.\n",
    "\n",
    "##### 1. Simple Exponential Smoothing\n",
    "\n",
    "- Most suited for a series that do not exhibit any trend or seasonality\n",
    "- We use a smoothing factor, $\\alpha$, to set the rate at which the weight assigned to past observations decay. It takes a value between $0$ and $1$. The higher the value of $\\alpha$, the more weight is assigned to recent observations.\n",
    "\n",
    "##### 2. Double Exponential Smoothing (a.k.a. Holt's Linear Trend Method)\n",
    "\n",
    "- Extension of the SES method to account for trend (but not seasonality) in a time series \n",
    "\n",
    "We now look at implementing exponential smoothing and forecasting on the Asian Paints stock.\n",
    "> *Asian Paints Limited, together with its subsidiaries, manufactures, sells, and distributes paints and coatings for decorative and industrial use in India and internationally. It operates in the Paints and Home Improvement segments.* - [Yahoo Finance](https://finance.yahoo.com/quote/ASIANPAINT.NS/profile?p=ASIANPAINT.NS).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end7 = datetime.date(2020, 6, 30)\n",
    "start7 = datetime.date(2010, 7, 1)\n",
    "ticker7 = \"ASIANPAINT.NS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(ticker7, start=start7, end=end7, progress=False)\n",
    "print(f\"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker7} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "################# PLEASE USE BELOW STATEMENTS IF NEEDED #####################\n",
    "#############################################################################\n",
    "\n",
    "## If you have don't have quandl, you can read the csv file as shown.\n",
    "\n",
    "# mydateparser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "# df.to_csv(f\"{ticker7}.csv\")\n",
    "# df4 = pd.read_csv(\"gold_prices_inr.csv\", index_col=0, parse_dates=True)\n",
    "# df1 = pd.read_csv(\"NSE_5min_interval.csv\", index_col=0, parse_dates=True, date_parser=mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"], inplace=True)\n",
    "df7.rename(columns = {'Adj Close': 'adj_close'}, inplace=True)\n",
    "print(df7.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "df7['adj_close'].plot(figsize=(12, 8), title=f\"{ticker7} adjusted close prices over time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the training & testing set\n",
    "\n",
    "train_length = int(np.round(0.9 * df7.shape[0]))\n",
    "df7_train = df7.iloc[:train_length]\n",
    "df7_test = df7.iloc[train_length:]\n",
    "test_length = len(df7_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.plot(title='Simple Exponential Smoothing', label='Actual', legend=True)\n",
    "\n",
    "ses_forecast_1.plot(legend=True, label=r'$\\alpha=0.2$')\n",
    "ses_1.fittedvalues.plot()\n",
    "\n",
    "ses_forecast_2.plot(legend=True, label=r'$\\alpha=0.5$')\n",
    "ses_2.fittedvalues.plot()\n",
    "\n",
    "ses_forecast_3.plot(legend=True, label=r'$\\alpha={0:.4f}$'.format(alpha))\n",
    "ses_3.fittedvalues.plot()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Simple Exponential Smoothing\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "\n",
    "## Creating the SES model class\n",
    "ses_model = SimpleExpSmoothing(df7_train)\n",
    "\n",
    "## Fitting models at different smoothing levels\n",
    "ses_fit_1 = ses_model.fit(smoothing_level=0)\n",
    "ses_fit_2 = ses_model.fit(smoothing_level=0.25)\n",
    "ses_fit_3 = ses_model.fit(smoothing_level=0.75)\n",
    "ses_fit_4 = ses_model.fit()\n",
    "alpha = ses_fit_4.model.params['smoothing_level']\n",
    "\n",
    "## Forecasting\n",
    "ses_fit_1_yhat = ses_fit_1.forecast(test_length)\n",
    "ses_fit_2_yhat = ses_fit_2.forecast(test_length)\n",
    "ses_fit_3_yhat = ses_fit_3.forecast(test_length)\n",
    "ses_fit_4_yhat = ses_fit_4.forecast(test_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses_fit_1_yhat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the results\n",
    "\n",
    "df7.plot(title='Simple Exponential Smoothing', label='Actual', legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'references'></a>\n",
    "#### References\n",
    "<a id = 'bnshephard'></a>\n",
    "<a id = 'arch'></a>\n",
    "<a id = 'others'></a>\n",
    "<a id = 'eryk'></a>\n",
    "<a id = 'cont2001'></a>\n",
    "\n",
    "1. Barndorff‐Nielsen, O. E., & Shephard, N. (2002). Econometric analysis of realized volatility and its use in estimating stochastic volatility models. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, 64(2), 253-280.\n",
    "2. Cochrane, John H. \"Time series for macroeconomics and finance.\" Manuscript, University of Chicago (2005).\n",
    "3. Cont, R.(2001). Empirical properties of asset returns: stylized facts and statistical issues.\n",
    "4. https://towardsdatascience.com/@eryk.lewinson\n",
    "5. https://pyflux.readthedocs.io/en/latest/getting_started.html\n",
    "6. https://tomaugspurger.github.io/modern-7-timeseries\n",
    "7. https://arch.readthedocs.io/en/latest/univariate/univariate_volatility_modeling.html\n",
    "6. Tsay, Ruey S. Analysis of financial time series. Vol. 543. John Wiley & Sons, 2005.\n",
    "7. Campbell, John Y., Andrew Wen-Chuan Lo, and Archie Craig MacKinlay. The Econometrics of Financial Markets. Vol. 2. Princeton, NJ: princeton University press, 1997."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
